#https://realpython.com/python-web-scraping-practical-introduction/
#https://www.crummy.com/software/BeautifulSoup/bs4/doc/

from requests import get
from requests.exceptions import RequestException
from contextlib import closing
from bs4 import BeautifulSoup
import msvcrt
import time
from datetime import datetime
import winsound
import csv

def simple_get(url):
    """
    Attempts to get the content at `url` by making an HTTP GET request.
    If the content-type of response is some kind of HTML/XML, return the
    text content, otherwise return None.
    """
    try:
        with closing(get(url, stream=True)) as resp:
            if is_good_response(resp):
                return resp.content
            else:
                return None

    except RequestException as e:
        log_error('Error during requests to {0} : {1}'.format(url, str(e)))
        return None

def is_good_response(resp):
    """
    Returns True if the response seems to be HTML, False otherwise.
    """
    content_type = resp.headers['Content-Type'].lower()
    return (resp.status_code == 200 
            and content_type is not None 
            and content_type.find('html') > -1)


def log_error(e):
    """
    It is always a good idea to log errors. 
    This function just prints them, but you can
    make it do anything.
    """
    print(e)    




def find_stuff(tofind):
	print(datetime.now().strftime('%Y-%m-%d %H:%M:%S') + ": Executing Search")
	#call function to get html data
	raw_html = simple_get(tofind)
	html = BeautifulSoup(raw_html, 'html.parser')


	#find records
	record_html = html.find_all(class_="cursor-pointer")
	if len(record_html) > 0:
		winsound.PlaySound("*", winsound.SND_ASYNC)

	else:
		print("No items found")
	
	record_string = []
	i=0
	while i<len(record_html):
		record_string.append(str(record_html[i]))
		i+=1

	record_string_split = []
	i=0
	while i<len(record_string):
		record_string_split.append(record_string[i].split("\n"))
		print(
		record_string_split[i][4].strip() + "\n" + #Item Name
		record_string_split[i][19].strip() + " --> " + #Location
		record_string_split[i][22].strip() + "\n" + #Guild
		record_string_split[i][27].strip() + " X " + #Unit Price
		record_string_split[i][33].strip() + " = " + #Units
		record_string_split[i][39].strip() +"\n" #Total Price
		)
		i+=1
		
stuff_to_find = []
with open("queries.csv") as f:
    for row in f:
        stuff_to_find.append(row.split(",")[1])

while 1:
	for x in stuff_to_find:
		find_stuff(x)
	time.sleep(60)

#__TODO__
#add functionality to only ping the first time it shows up - maybe log trade IDs to an array and change  find stuff to include check that trade id not in array
#Append column 1 to item alias array and print(item_alias + " not found")

